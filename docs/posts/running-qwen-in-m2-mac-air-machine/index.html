<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Running Qwen in M2 Mac Air Machine using Ollama | teraskula.com</title>

<meta name="keywords" content="technology, commit2, ai" />
<meta name="description" content="Running qwen-2.5-coder:7B on macbook Air M2 using ollama This is part of stay hungry stay folish mindset, and my interest in AI. Search possible solution to get cheapest code assistant as possible. i was subscribe to copilot but now after found continou.dev &#43; anthropic API, thats the current choice.
Its small decrease in cost. copilot is $10 per month but now i am not that code heavy so subs to token based payment (anthropic claude 3.">
<meta name="author" content="">
<link rel="canonical" href="https://teraskula.com/posts/running-qwen-in-m2-mac-air-machine/" />
<link href="/assets/css/stylesheet.min.746a86b58bb2b052b5e4df8216510494f04f81e62c08d626150c26c69ca929da.css" integrity="sha256-dGqGtYuysFK15N&#43;CFlEElPBPgeYsCNYmFQwmxpypKdo=" rel="preload stylesheet"
    as="style">

<link rel="icon" href="https://teraskula.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://teraskula.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://teraskula.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://teraskula.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://teraskula.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.126.1">
<link rel="alternate" hreflang="en" href="https://teraskula.com/posts/running-qwen-in-m2-mac-air-machine/" />


<meta property="og:title" content="Running Qwen in M2 Mac Air Machine using Ollama" />
<meta property="og:description" content="Running qwen-2.5-coder:7B on macbook Air M2 using ollama This is part of stay hungry stay folish mindset, and my interest in AI. Search possible solution to get cheapest code assistant as possible. i was subscribe to copilot but now after found continou.dev &#43; anthropic API, thats the current choice.
Its small decrease in cost. copilot is $10 per month but now i am not that code heavy so subs to token based payment (anthropic claude 3." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://teraskula.com/posts/running-qwen-in-m2-mac-air-machine/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-11-20T00:57:25&#43;07:00" />
<meta property="article:modified_time" content="2024-11-20T00:57:25&#43;07:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Running Qwen in M2 Mac Air Machine using Ollama"/>
<meta name="twitter:description" content="Running qwen-2.5-coder:7B on macbook Air M2 using ollama This is part of stay hungry stay folish mindset, and my interest in AI. Search possible solution to get cheapest code assistant as possible. i was subscribe to copilot but now after found continou.dev &#43; anthropic API, thats the current choice.
Its small decrease in cost. copilot is $10 per month but now i am not that code heavy so subs to token based payment (anthropic claude 3."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://teraskula.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Running Qwen in M2 Mac Air Machine using Ollama",
      "item": "https://teraskula.com/posts/running-qwen-in-m2-mac-air-machine/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Running Qwen in M2 Mac Air Machine using Ollama",
  "name": "Running Qwen in M2 Mac Air Machine using Ollama",
  "description": "Running qwen-2.5-coder:7B on macbook Air M2 using ollama This is part of stay hungry stay folish mindset, and my interest in AI. Search possible solution to get cheapest code assistant as possible. i was subscribe to copilot but now after found continou.dev + anthropic API, thats the current choice.\nIts small decrease in cost. copilot is $10 per month but now i am not that code heavy so subs to token based payment (anthropic claude 3.",
  "keywords": [
    "technology", "commit2", "ai"
  ],
  "articleBody": "Running qwen-2.5-coder:7B on macbook Air M2 using ollama This is part of stay hungry stay folish mindset, and my interest in AI. Search possible solution to get cheapest code assistant as possible. i was subscribe to copilot but now after found continou.dev + anthropic API, thats the current choice.\nIts small decrease in cost. copilot is $10 per month but now i am not that code heavy so subs to token based payment (anthropic claude 3.5 sonnet) is more cost effective. and py base on what i use.\nby the way, this blog is write by myself ya. :p\nidea Now in the search of the Cheapest one, my mind was :\nwow if i could run llm locally and make continou.dev hitting the local llm, that will be good right?\nit will be FREEEEEEEE Model Search ok, now when searching the newest model for coding, there is HOT model out there qwen-2.5-coder that just recently open sourced. thanks to Alibaba Cloud that develop and make it available to public.\nso decided qwen-2.5-7b is the target model that i want to implement.\nTooling when face with tooling, i already using ollama before and just use that. ollama is tooling to bridge llm to you. ollama is interface that will communicate with the llm.\nstart ollama by\nollama serve\nand in another terminal start\nollama run qwen2.5-coder:7b\nthats all thats an AI on your local or in this case is my M2 macbook air.\nquestion and answer for general chat, pretty much the same. I am not bencmarking here, for generating simple code are oke. the next step is to make that as default model in continou.dev\nintegrate with continou.dev from config on continou.dev in visualcode studio open the config with cmd+sift+p search for continou:config\n{ \"models\": [ { \"model\": \"qwen2.5-coder:7b\", \"provider\": \"ollama\", \"title\": \"qwen2.5-coder-7b\" } ] } its working but Macbook M2 are freezing even the touchbar sometimes its freeze and cannot be clicked\nand in this state i cancel all process to free the RAM\nThe Realization When running as chat in command line the llm are working fine. slow but still ok. When try in visualcode studio as code assistant its kind of freeze, work but extreamly slow. token based API + continou still the best and the most effective (for me). summary step by step Install and Set Up Ollama Test the Model (ollama run ) Integrate with Continue.dev enjoy and realize that how complex is LLM that even M2 is strugling. thank you\n",
  "wordCount" : "417",
  "inLanguage": "en",
  "datePublished": "2024-11-20T00:57:25+07:00",
  "dateModified": "2024-11-20T00:57:25+07:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://teraskula.com/posts/running-qwen-in-m2-mac-air-machine/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "teraskula.com",
    "logo": {
      "@type": "ImageObject",
      "url": "https://teraskula.com/favicon.ico"
    }
  }
}
</script>




</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }
    </style>

</noscript>
<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://teraskula.com/" accesskey="h" title="teraskula.com (Alt + H)">teraskula.com</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                
                

                <ul class="lang-switch"><li>|</li>
                </ul>
            </span>
        </div>
        <ul id="menu" onscroll="menu_on_scroll()">
            <li>
                <a href="https://teraskula.com/posts/about-teraskula/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://teraskula.com/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://teraskula.com/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li></ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">

    <h1 class="post-title">
      Running Qwen in M2 Mac Air Machine using Ollama
    </h1>
    <div class="post-meta">November 20, 2024


</div>
  </header> 

  <div class="post-content">
<h3 id="running-qwen-25-coder7b-on-macbook-air-m2-using-ollama">Running qwen-2.5-coder:7B on macbook Air M2 using ollama<a hidden class="anchor" aria-hidden="true" href="#running-qwen-25-coder7b-on-macbook-air-m2-using-ollama">#</a></h3>
<p>This is part of stay hungry stay folish mindset, and my interest in AI. Search possible solution to get cheapest code assistant as possible. i was subscribe to copilot but now after found <a href="https://www.continue.dev/">continou.dev</a> + <a href="https://www.anthropic.com/pricing#anthropic-api">anthropic</a> API, thats the current choice.</p>
<p>Its small decrease in cost. copilot is $10 per month but now i am not that code heavy so subs to token based payment (anthropic claude 3.5 sonnet) is more cost effective. and py base on what i use.</p>
<blockquote>
<p><em>by the way, this blog is <strong>write</strong> by myself ya. :p</em></p>
</blockquote>
<h3 id="idea">idea<a hidden class="anchor" aria-hidden="true" href="#idea">#</a></h3>
<p>Now in the search of the <strong>Cheapest</strong> one,
my mind was :</p>
<p><code>wow if i could run llm locally and make continou.dev hitting the local llm, that will be good right?</code></p>
<p>it will be FREEEEEEEE
<img loading="lazy" src="/img/qwen-m2-1.png" alt="IT WILL BE FREE"  />
</p>
<h3 id="model-search">Model Search<a hidden class="anchor" aria-hidden="true" href="#model-search">#</a></h3>
<p>ok, now when searching the newest model for coding, there is HOT model out there qwen-2.5-coder that just recently open sourced. thanks to <a href="https://www.alibabacloud.com/blog/601765">Alibaba Cloud</a> that develop and make it available to public.</p>
<p>so decided <em>qwen-2.5-7b</em> is the target model that i want to implement.</p>
<h3 id="tooling">Tooling<a hidden class="anchor" aria-hidden="true" href="#tooling">#</a></h3>
<p>when face with tooling, i already using <a href="https://ollama.com/">ollama</a> before and just use that. ollama is tooling to bridge llm to you. ollama is interface that will communicate with the llm.</p>
<p>start ollama by</p>
<p><code>ollama serve</code></p>
<p>and in another terminal start</p>
<p><code>ollama run qwen2.5-coder:7b</code></p>
<p><img loading="lazy" src="/img/qwen-m2-2.png" alt="IT WILL BE FREE"  />
</p>
<p>thats all thats an AI on your local or in this case is my M2 macbook air.</p>
<p>question and answer for general chat, pretty much the same. I am not bencmarking here, for generating simple code are oke. the next step is to make that as default model in continou.dev</p>
<p><img loading="lazy" src="/img/qwen-m2-3.png" alt="IT WILL BE FREE"  />
</p>
<h3 id="integrate-with-continoudev">integrate with continou.dev<a hidden class="anchor" aria-hidden="true" href="#integrate-with-continoudev">#</a></h3>
<p>from config on continou.dev in visualcode studio open the config with <code>cmd+sift+p</code> search for <code>continou:config</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;models&#34;</span>: [
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;model&#34;</span>: <span style="color:#e6db74">&#34;qwen2.5-coder:7b&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;provider&#34;</span>: <span style="color:#e6db74">&#34;ollama&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&#34;title&#34;</span>: <span style="color:#e6db74">&#34;qwen2.5-coder-7b&#34;</span>
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>its working but Macbook M2 are freezing even the touchbar sometimes its freeze and cannot be clicked</p>
<p>and in this state i cancel all process to free the RAM</p>
<p><img loading="lazy" src="/img/qwen-m2-4.png" alt="IT WILL BE FREE"  />
</p>
<h2 id="the-realization">The Realization<a hidden class="anchor" aria-hidden="true" href="#the-realization">#</a></h2>
<ul>
<li>When running as chat in command line the llm are working fine. slow but still ok.</li>
<li>When try in visualcode studio as code assistant its kind of freeze, work but extreamly slow.</li>
<li>token based API + continou still the best and the most effective (for me).</li>
</ul>
<h2 id="summary-step-by-step">summary step by step<a hidden class="anchor" aria-hidden="true" href="#summary-step-by-step">#</a></h2>
<ul>
<li>Install and Set Up Ollama</li>
<li>Test the Model (ollama run <!-- raw HTML omitted -->)</li>
<li>Integrate with Continue.dev</li>
<li>enjoy and realize that how complex is LLM that even M2 is strugling.</li>
</ul>
<p>thank you</p>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://teraskula.com/tags/technology/">Technology</a></li>
      <li><a href="https://teraskula.com/tags/commit2/">Commit2</a></li>
      <li><a href="https://teraskula.com/tags/ai/">Ai</a></li>
    </ul>
  </footer>

<div id="disqus_thread"></div>
<script>
    

    
    var disqus_config = function () {
    this.page.url = 'https:\/\/teraskula.com\/posts\/running-qwen-in-m2-mac-air-machine\/';  
    this.page.identifier = 'UnVubmluZyBRd2VuIGluIE0yIE1hYyBBaXIgTWFjaGluZSB1c2luZyBPbGxhbWE='; 
    };
    
    (function() { 
    var d = document, s = d.createElement('script');
    s.src = 'https://prima101112.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

</article>
    </main><footer class="footer">
    <span>&copy; 2024 <a href="https://teraskula.com/">teraskula.com</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a></span>
    <span>&middot;</span>
    <span>Theme <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>



<script defer src="/assets/js/highlight.min.2eadbb982468c11a433a3e291f01326f2ba43f065e256bf792dbd79640a92316.js" integrity="sha256-Lq27mCRowRpDOj4pHwEybyukPwZeJWv3ktvXlkCpIxY="
    onload="hljs.initHighlightingOnLoad();"></script>
<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }

    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>

</body>

</html>
