<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-99V7B3NL74"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-99V7B3NL74');
</script>
<title>Langchain PDF Ollama RAG (Retrieval Augmented Generation) | teraskula.com</title>
<meta name="keywords" content="technology, ai, nextjs">
<meta name="description" content="Give PDF and talk about it
using lanchain to perform RAG using Ollama embedings and FAISS vectorstore
yeah maybe many tools already provide this kind of feature, But this is different, this is about knowing behind the scene. what actually they do to the pdf files? what actually we do to the text in it? and how the LLM is know what context they need?.
first thing first what is RAG? RAG is Retrieval Augmented Generation">
<meta name="author" content="prima adi">
<link rel="canonical" href="http://localhost:1313/posts/langchain-pdf-ollama-rag/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css" integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/langchain-pdf-ollama-rag/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="teraskula.com (Alt + H)">teraskula.com</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/about-teraskula/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>

<script type="module">
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
    mermaid.initialize({ startOnLoad: true });
</script>


</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Langchain PDF Ollama RAG (Retrieval Augmented Generation)
    </h1>
    <div class="post-meta">2 December 2024 | prima adi

</div>
  </header> 
  <div class="post-content"><h2 id="give-pdf-and-talk-about-it">Give PDF and talk about it<a hidden class="anchor" aria-hidden="true" href="#give-pdf-and-talk-about-it">#</a></h2>
<h4 id="using-lanchain-to-perform-rag-using-ollama-embedings-and-faiss-vectorstore">using lanchain to perform RAG using Ollama embedings and FAISS vectorstore<a hidden class="anchor" aria-hidden="true" href="#using-lanchain-to-perform-rag-using-ollama-embedings-and-faiss-vectorstore">#</a></h4>
<p>yeah maybe many tools already provide this kind of feature, But this is different, this is about knowing behind the scene. what actually they do to the pdf files? what actually we do to the text in it? and how the LLM is know what context they need?.</p>
<p>first thing first what is <strong>RAG</strong>? RAG is <strong>Retrieval Augmented Generation</strong></p>
<p><strong>RAG</strong> is like asking a person who doesn’t just rely on memory (llms default) but goes to check a book (pdf in this case) or notes before answering. First, it finds the right information (retrieval), then it uses that to give us a clear answer (generation). It’s smarter and targeted.</p>
<p>In this state we are gonna learn <strong>langchain</strong> that will get pdf files and we ask something about it and basically perform RAG. <em>langchain_community is in awesome growth</em>.</p>
<p>lets go:</p>
<p><a href="https://api.python.langchain.com/en/latest/langchain_api_reference.html">LANGCHAIN PYTHON API REFERENCE</a></p>
<p>thats reference is make us easy to refer what the function or method is actually do.</p>
<p>this is the basic flow chart from what we are gonna build</p>
<div class="mermaid">
graph TD
    A[User Input: PDF Document] --&gt;|Preprocess PDF| B[Extracted Text]
    B --&gt;|Embed Text| C[Document Embeddings]
    C --&gt;|Store Embeddings| D[FAISS Vector Database]
    E[User Query] --&gt;|Input Query| F[Vector Database Search]
    D --&gt;|Retrieve Matching Vectors| F
    subgraph ide1 [rag]
    F --&gt;|Find Matching Vectors| G[Relevant Document Embeddings]
    G --&gt;|Convert Vectors to Text| H(((&#34;`**Retrieved Text Documents**`&#34;)))
    H --&gt;|Provide Context| I[Language Model LLM]
    end
    I --&gt;|Generate Response| J[Final Answer]
</div>
<p>what we are gonna do is 3 step.</p>
<h4 id="1-load-pdf-files-and-split-to-chunks">1. Load pdf files and split to chunks<a hidden class="anchor" aria-hidden="true" href="#1-load-pdf-files-and-split-to-chunks">#</a></h4>
<p>load the pdf file. since we are using langchain in the langchain_community there is <a href="https://python.langchain.com/docs/integrations/document_loaders/pypdfloader/">pdfloader</a>. and split the documents to chunks.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Step 1: Load the PDF</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load_pdf</span>(pdf_path):
</span></span><span style="display:flex;"><span>    loader <span style="color:#f92672">=</span> PyPDFLoader(pdf_path)
</span></span><span style="display:flex;"><span>    documents <span style="color:#f92672">=</span> loader<span style="color:#f92672">.</span>load()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> documents
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Split the text into chunks</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">split_text</span>(documents):
</span></span><span style="display:flex;"><span>    text_splitter <span style="color:#f92672">=</span> RecursiveCharacterTextSplitter(
</span></span><span style="display:flex;"><span>        chunk_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>,
</span></span><span style="display:flex;"><span>        chunk_overlap<span style="color:#f92672">=</span><span style="color:#ae81ff">200</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> text_splitter<span style="color:#f92672">.</span>split_documents(documents)
</span></span></code></pre></div><h4 id="2-do-embedings---vectorstores">2. do embedings -&gt; vectorstores<a hidden class="anchor" aria-hidden="true" href="#2-do-embedings---vectorstores">#</a></h4>
<p>we need to <strong>split the pdf text to chunks</strong> so it could translate our docs to <strong>vecrorstore</strong>. we could say its a data that could understand by the LLM&rsquo;s. and because we not yet big lets use vectorestore FAISS using in memory. In python and in this AI world. everything is already wraped. but if you want to learn more <a href="https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.faiss.FAISS.html#langchain_community.vectorstores.faiss.FAISS">FAISS here</a> also there is a paper in it <a href="https://arxiv.org/pdf/2401.08281">faiss paper</a></p>
<p>the <strong>embedding</strong>: since we use ollama we use OllamaEmbeddings there should be OpenAIEmbbedings, also
full docs could read <a href="https://python.langchain.com/v0.2/api_reference/ollama/embeddings/langchain_ollama.embeddings.OllamaEmbeddings.html#langchain_ollama.embeddings.OllamaEmbeddings">here ollama embedings</a></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Step 2: generate embeddings using OllamaEmbedings</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_vectorstore</span>(chunks):
</span></span><span style="display:flex;"><span>    embeddings <span style="color:#f92672">=</span> OllamaEmbeddings(model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;mistral&#34;</span>) <span style="color:#75715e">#just use mistral its run on local btw</span>
</span></span><span style="display:flex;"><span>    vectorstore <span style="color:#f92672">=</span> FAISS<span style="color:#f92672">.</span>from_documents(chunks, embeddings)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> vectorstore
</span></span></code></pre></div><p>we also could use another vectorstore but now we use FAISS</p>
<h4 id="3-build-the-rag-chains">3. build the RAG chains<a hidden class="anchor" aria-hidden="true" href="#3-build-the-rag-chains">#</a></h4>
<p>Build the RAG pipeline. honestly i am using AI to find this code cause reading this still strugling <a href="https://python.langchain.com/v0.1/docs/modules/chains/">chain</a> there is many chain and i am using the <code>RetrievalQA</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">build_rag</span>(vectorstore):
</span></span><span style="display:flex;"><span>    retriever <span style="color:#f92672">=</span> vectorstore<span style="color:#f92672">.</span>as_retriever()
</span></span><span style="display:flex;"><span>    llm <span style="color:#f92672">=</span> OllamaLLM(model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;mistral&#34;</span>)  <span style="color:#75715e"># Specify Mistral model</span>
</span></span><span style="display:flex;"><span>    qa_chain <span style="color:#f92672">=</span> RetrievalQA<span style="color:#f92672">.</span>from_chain_type(llm, retriever<span style="color:#f92672">=</span>retriever)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> qa_chain
</span></span></code></pre></div><h4 id="combine-that-together-as-qa-system-prompt">Combine that together as Q&amp;A system prompt.<a hidden class="anchor" aria-hidden="true" href="#combine-that-together-as-qa-system-prompt">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Combine all to main</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">main</span>():
</span></span><span style="display:flex;"><span>    pdf_path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/Users/prima/Downloads/jefferey-ollama.pdf&#34;</span>  <span style="color:#75715e"># Path to your PDF file</span>
</span></span><span style="display:flex;"><span>    documents <span style="color:#f92672">=</span> load_pdf(pdf_path)
</span></span><span style="display:flex;"><span>    chunks <span style="color:#f92672">=</span> split_text(documents)
</span></span><span style="display:flex;"><span>    vectorstore <span style="color:#f92672">=</span> create_vectorstore(chunks)
</span></span><span style="display:flex;"><span>    qa_chain <span style="color:#f92672">=</span> build_rag(vectorstore)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Chat with the PDF</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>        query <span style="color:#f92672">=</span> input(<span style="color:#e6db74">&#34;Ask a question about the document: &#34;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> query<span style="color:#f92672">.</span>lower() <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;exit&#34;</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>        response <span style="color:#f92672">=</span> qa_chain<span style="color:#f92672">.</span>invoke(query)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Answer: </span><span style="color:#e6db74">{</span>response<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><h2 id="result-and-learnings">Result and Learnings<a hidden class="anchor" aria-hidden="true" href="#result-and-learnings">#</a></h2>
<p>with this we are able to ask something about the pdf that we are give to the system. in this <em>example</em> i am creating pdf files base on <strong>Jeffrey Morgan</strong> linked in profile export pdf <a href="https://www.linkedin.com/in/jmorganca/?originalSubdomain=ca">jeffrey Morgan</a>.</p>
<h4 id="chatgpt-not-mentioning-ollama-jeffrey-morgan-is">chatgpt (not mentioning ollama) jeffrey morgan is:<a hidden class="anchor" aria-hidden="true" href="#chatgpt-not-mentioning-ollama-jeffrey-morgan-is">#</a></h4>
<p>jeffrey is an actor</p>
<p><img loading="lazy" src="/img/langchain-rag-ollama-1.png" alt="langchain ollama rag"  />
</p>
<h4 id="olama-mistral-without-pdf-clean-mistral-not-using-rag">olama mistral without pdf (clean mistral) not using RAG<a hidden class="anchor" aria-hidden="true" href="#olama-mistral-without-pdf-clean-mistral-not-using-rag">#</a></h4>
<p>jeffrey is an actor</p>
<p><img loading="lazy" src="/img/langchain-rag-ollama-4.png" alt="langchain ollama rag"  />
</p>
<h4 id="when-i-ask-our-rag-ollama-and-ask-about-jefferey-morgan-the-reult-is-success">when i ask our RAG ollama and ask about jefferey morgan the reult is success:<a hidden class="anchor" aria-hidden="true" href="#when-i-ask-our-rag-ollama-and-ask-about-jefferey-morgan-the-reult-is-success">#</a></h4>
<p>jeffrey is an engineer</p>
<p><img loading="lazy" src="/img/langchain-rag-ollama-3.png" alt="langchain ollama rag"  />
</p>
<p>this is the full code and enjoy</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_community.document_loaders <span style="color:#f92672">import</span> PyPDFLoader
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.text_splitter <span style="color:#f92672">import</span> RecursiveCharacterTextSplitter
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_ollama <span style="color:#f92672">import</span> OllamaLLM, OllamaEmbeddings
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_community.vectorstores <span style="color:#f92672">import</span> FAISS
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.chains <span style="color:#f92672">import</span> RetrievalQA
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Step 1: Load the PDF</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load_pdf</span>(pdf_path):
</span></span><span style="display:flex;"><span>    loader <span style="color:#f92672">=</span> PyPDFLoader(pdf_path)
</span></span><span style="display:flex;"><span>    documents <span style="color:#f92672">=</span> loader<span style="color:#f92672">.</span>load()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> documents
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Split the text into chunks</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">split_text</span>(documents):
</span></span><span style="display:flex;"><span>    text_splitter <span style="color:#f92672">=</span> RecursiveCharacterTextSplitter(
</span></span><span style="display:flex;"><span>        chunk_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>,
</span></span><span style="display:flex;"><span>        chunk_overlap<span style="color:#f92672">=</span><span style="color:#ae81ff">200</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> text_splitter<span style="color:#f92672">.</span>split_documents(documents)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Step 2: generate embeddings using OllamaEmbedings</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_vectorstore</span>(chunks):
</span></span><span style="display:flex;"><span>    embeddings <span style="color:#f92672">=</span> OllamaEmbeddings(model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;mistral&#34;</span>) <span style="color:#75715e">#just use mistral its run on local btw</span>
</span></span><span style="display:flex;"><span>    vectorstore <span style="color:#f92672">=</span> FAISS<span style="color:#f92672">.</span>from_documents(chunks, embeddings)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> vectorstore
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Step 3: Build the RAG system</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">build_rag</span>(vectorstore):
</span></span><span style="display:flex;"><span>    retriever <span style="color:#f92672">=</span> vectorstore<span style="color:#f92672">.</span>as_retriever()
</span></span><span style="display:flex;"><span>    llm <span style="color:#f92672">=</span> OllamaLLM(model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;mistral&#34;</span>)  <span style="color:#75715e"># Specify Mistral model</span>
</span></span><span style="display:flex;"><span>    qa_chain <span style="color:#f92672">=</span> RetrievalQA<span style="color:#f92672">.</span>from_chain_type(llm, retriever<span style="color:#f92672">=</span>retriever)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> qa_chain
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Combine all to main</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">main</span>():
</span></span><span style="display:flex;"><span>    pdf_path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/Users/prima/Downloads/jefferey-ollama.pdf&#34;</span>  <span style="color:#75715e"># Path to your PDF file</span>
</span></span><span style="display:flex;"><span>    documents <span style="color:#f92672">=</span> load_pdf(pdf_path)
</span></span><span style="display:flex;"><span>    chunks <span style="color:#f92672">=</span> split_text(documents)
</span></span><span style="display:flex;"><span>    vectorstore <span style="color:#f92672">=</span> create_vectorstore(chunks)
</span></span><span style="display:flex;"><span>    qa_chain <span style="color:#f92672">=</span> build_rag(vectorstore)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Chat with the PDF</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>        query <span style="color:#f92672">=</span> input(<span style="color:#e6db74">&#34;Ask a question about the document: &#34;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> query<span style="color:#f92672">.</span>lower() <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;exit&#34;</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>        response <span style="color:#f92672">=</span> qa_chain<span style="color:#f92672">.</span>invoke(query)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Answer: </span><span style="color:#e6db74">{</span>response<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;__main__&#34;</span>:
</span></span><span style="display:flex;"><span>    main()
</span></span></code></pre></div><blockquote>
<p>make sure have pip and python also install all requirements</p>
</blockquote>
<blockquote>
<p>langchain has move their community module to <code>langchain_community</code> since they are have enterprise package and products.</p>
</blockquote>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/technology/">Technology</a></li>
      <li><a href="http://localhost:1313/tags/ai/">Ai</a></li>
      <li><a href="http://localhost:1313/tags/nextjs/">Nextjs</a></li>
    </ul>
  </footer><div id="disqus_thread"></div>
<script>
    

    

    (function() { 
    var d = document, s = d.createElement('script');
    s.src = 'https://prima101112.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="http://localhost:1313/">teraskula.com</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
